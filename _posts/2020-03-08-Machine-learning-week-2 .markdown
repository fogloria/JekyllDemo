---
layout: post
title:  "Machine learning week #2 THR ROC"
date:   2020-03-08 23:55:25 +0800
categories: 
---
# 机器学习Week 2：如何评价模型的好坏

所谓超参数，就是在机器学习算法模型执行之前需要指定的参数。（调参调的就是超参数） 如kNN算法中的k。

与之相对的概念是模型参数，即算法过程中学习的属于这个模型的参数（kNN中没有模型参数，回归算法有很多模型参数）

因为有两个超参数，因此使用双重循环，去查找最合适的两个参数

我们会注意到，`best_estimator_`和`best_score_`参数后面有一个`_`。这是一种常见的语法规范，不是用户传入的参数，而是根据用户传入的规则，自己计算出来的结果，参数名字后面接`_`

网格搜索Grid Serach就是把参数都验证过去

对回归问题的评价：MSE、RMSE、MAE、R方

混淆矩阵(Confusion Matrix)

**精准率（查准率）precision：预测值为1，且预测对了的比例，即：我们关注的那个事件，预测的有多准。**

**召回率（查全率）recall：所有真实值为1的数据中，预测对了的个数，即：我们关注的那个事件真实的发生情况下，我们成功预测的比例是多少。**

视场景兼顾**调和**平均值F1 Score:
$$
F1=\frac {2*precision*recall}{precision+recall}
$$

$$
\frac {1}{F1}=\frac12(\frac1{precision} +\frac1{recall})
$$


thr(阈值要求)始终应该是0.5吗？

分类阈值取不同值，TPR和FPR的计算结果也不同，最理想情况下，我们希望所有正例 & 负例 都被成功预测  TPR=1，FPR=0，即 所有的正例预测值 > 所有的负例预测值，此时阈值取最小正例预测值与最大负例预测值之间的值即可。

关于ROC，x轴是FPR，y轴是TPR，TPR>FPR，即上班平面判断越准确；

在ROC曲线中，曲线下的面积AUC越大，分类效果越好。





