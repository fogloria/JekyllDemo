---
layout: post
title:  "ML week #7 Decision Tree"
date:   2020-04-14 17:58:06 +0800
categories: 
---
## 决策树相关概念及模型算法推导

建模思路是尽可能模拟人做决策的过程，在学术上归为白盒模型。

决策树表示给定特征条件下，类的条件概率分布，这个条件概率分布表示在特征空间的划分上，将特征空间根据各个特征值不断进行划分，就将特征空间分为了多个不相交的单元，在每个单元定义了一个类的概率分布，这样，这条由根节点到达叶节点的路径就成了一个条件概率分布。

**决策树学习本质上是从训练数据集中归纳出一组分类规则**。需要找到一个**与训练数据矛盾较小的决策树，同时具有很好的泛化能力**。**决策树学习是由训练数据集估计条件概率模型**。

### 决策树损失函数

决策树学习的损失函数通常是正则化的极大似然函数。

### 决策树的构建

1. 开始：构建根节点，**将所有训练数据都放在根节点，选择一个最优特征**，按照这一特征将训练数据集**分割**成子集，使得各个子集有一个在当前条件下最好的分类。
2. 如果这些子集已经能够被基本正确分类，那么构建叶节点，并**将这些子集分到所对应的叶子节点**去。
3. 如果还有子集不能够被正确的分类，那么就**对这些子集选择新的最优特征，继续对其进行分割**，构建相应的节点，**如此递归进行，直至所有训练数据子集被基本正确的分类，或者没有合适的特征为止**。
4. **每个子集都被分到叶节点上，即都有了明确的类**，这样就生成了一颗决策树。

这样生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象。我们需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使其具有更好的泛化能力。

决策树生成和决策树剪枝是个相对的过程，决策树生成旨在得到对于当前子数据集最好的分类效果(局部最优)，而决策树剪枝则是考虑全局最优，增强泛化能力。

#### 构建决策树时的特征选择

直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。

首先找到一个维度，然后在维度上找到一个阈值。然后以这个维度的这个阈值为依据进行划分。



### 信息熵

对于信息熵来说，只有两个类别，其中一个类别是0.5，另一个类别是1-0.5时，此时信息熵是最大的，也就是最不确定的。如果x偏向于某一类，确定性变高了，信息熵变低了。

### 条件熵

条件熵H（Y|X）表示**在已知随机变量X的条件下随机变量Y的不确定性**。**与信息熵不同的是，条件熵是数学期望，而不是变量的不确定性。**

### 信息增益

在划分数据集前后**信息发生的变化**称为信息增益，**获得信息增益最高的特征就是最好的选择**。

信息增益偏向于选择取值较多的特征，容易过拟合，因此需要“信息增益率”，是在信息增益的基础之上乘上一个惩罚参数，对树分支过多的情况进行惩罚，抵消了特征变量的复杂程度，避免了过拟合的存在。

信息增益比 = 惩罚参数 * 信息增益（信息增益总是偏向于选择取值较多的属性。信息增益比在此基础上增加了一个罚项，解决了这个问题。）

惩罚参数，**是数据集D以特征A作为随机变量的熵的倒数，即：将特征A取值相同的样本划分到同一个子集中**



### 基尼系数

基尼系数（Gini），表示在样本集合中一个随机选中的样本**被分错的概率**。

> 基尼指数（基尼不纯度）= 样本被选中的概率 * 样本被分错的概率
> $$
> Gini(p)=\Sigma p_k(1-p_k)=1-\Sigma p_k^2
> $$
> 当二分类时，G=2p(1-p)



## ID3、C4.5、CART决策树代码实现

### ID3

通过计算每个特征的**信息增益**，每次划分选取信息增益最高的属性为划分标准，**递归**地构建决策树。

### C4.5

C4.5算法与ID3算法过程相似，仅在特征选择时，使用**信息增益比**作为特征选择准则。



比较：

>ID3 仅仅适用于二分类问题。ID3 仅仅能够处理离散属性。
>
>C4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。



决策树非常容易产生过拟合，实际所有非参数学习算法，都非常容易产生过拟合。

### 决策树的修剪

**预剪枝**是指在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点标记为叶节点。预剪枝决策树有可能带来**欠拟合**的风险。

**后剪枝**是先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树完全替换为叶节点能带来决策树繁花性的提升，则将该子树替换为叶节点。

后剪枝决策树通常比预剪枝决策树保留了更多的分支，一般情形下，后剪枝决策树的欠拟合风险小，**泛化性能往往也要优于预剪枝决策树**。但后剪枝过程是在构建完全决策树之后进行的，并且要自底向上的对树中的所有非叶结点进行逐一考察，因此其**训练时间**开销要比未剪枝决策树和预剪枝决策树都大得多。

### CART

CART算法：Classification And Regression Tree。顾名思义，CART算法既可以用于创建分类树（Classification Tree），也可以用于创建回归树（Regression Tree）、模型树（Model Tree），两者在建树的过程稍有差异。既可以解决分类问题，也可以解决回归问题。**根据某一个维度d和某一个阈值v进行二分，得到的决策树是二叉树**。





ref:

[决策树1：初识决策树](http://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484178&idx=1&sn=1af90ba53ee3c1036ca94be9180e4c81&chksm=eb932aa4dce4a3b23aaed46d779abbc00917a55ac71cf177737c462ef9a02732e087049d132a&scene=21#wechat_redirect)

[决策树2: 特征选择中的相关概念](http://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484195&idx=1&sn=054651159e08f74c85f1fc1fab8a0f25&chksm=eb932a95dce4a383870f5385e9e926af022ecb5f7c9b827c2fb4c72af4f66a1133b5a88d3cb2&scene=21#wechat_redirect)

[决策树3: 特征选择之寻找最优划分](http://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484204&idx=1&sn=372d3ee90802d15347445f91056fe6bc&chksm=eb932a9adce4a38c9750a2cf6ca3382056b66099476223da02016e964ef711b97c4798ba9e58&scene=21#wechat_redirect)

[决策树4：构建算法之ID3、C4.5](http://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484212&idx=1&sn=aaabacf77db62a2b78fbef5ad034e354&chksm=eb932a82dce4a394fbf447fdd01b167565f04ddf85db5ef5e969bc0c75bd0dbf67b36236d64b&scene=21#wechat_redirect)

[决策树5：剪枝与sklearn中的决策树](http://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484241&idx=1&sn=fa915fb55d98b38c54674a58f18ba921&chksm=eb932ae7dce4a3f16423bc2e61ca5f819b482e438df3025bcac6dd9d9c07ccf6424fc79463a9&scene=21#wechat_redirect)

[决策树6：分类与回归树CART](http://mp.weixin.qq.com/s?__biz=MzI4MjkzNTUxMw==&mid=2247484269&idx=1&sn=a0c39116207b0d299c48764640d4c582&chksm=eb932adbdce4a3cd0a962b735ffb3fe5bad1739259ced35f3d1e03b8eee05db5cd9fb025395c&scene=21#wechat_redirect)

