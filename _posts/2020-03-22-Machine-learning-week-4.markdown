---
layout: post
title:  "ML week #4 LinearCongression"
date:   2020-03-22 11:34:00 +0800
categories: 
---
## æœºå™¨å­¦ä¹ week 4: çº¿æ€§å›žå½’ç®—æ³•æ˜¯æ¨¡åž‹ä¹‹æ¯

#### æŸå¤±å‡½æ•°

æœ€å°äºŒä¹˜æ³•ï¼ˆæœ€å°åŒ–è¯¯å·®çš„å¹³æ–¹ï¼‰
$$
a=\frac{\sum_{i=1}^m (x_i-\overline{x})\times(y_i-\overline{y})}{\sum_{i=1}^m({x_i-\overline{x}})^2}
$$

$$
b=\overline{y}-a\overline{x}
$$

çº¿æ€§ï¼šæ–¹ç¨‹æ˜¯çº¿æ€§ï¼ˆä¸€æ¬¡å‡½æ•°ï¼‰çš„ï¼›

å›žå½’ï¼šç”¨æ–¹ç¨‹æ¥æ¨¡æ‹Ÿå˜é‡ä¹‹é—´æ˜¯å¦‚ä½•å…³è”çš„ã€‚

ç»“æžœæœ‰å¾ˆå¥½çš„å¯è§£é‡Šæ€§

éœ€è¦ä¸€æ¡ç›´çº¿ï¼Œæœ€å¤§ç¨‹åº¦çš„æ‹Ÿåˆæ ·æœ¬ç‰¹å¾å’Œæ ·æœ¬æ•°æ®æ ‡è®°ä¹‹é—´çš„å…³ç³»ã€‚

å»ºæ¨¡è¿‡ç¨‹ï¼Œå°±æ˜¯æ‰¾åˆ°ä¸€ä¸ªæ¨¡åž‹ï¼Œæœ€å¤§ç¨‹åº¦çš„æ‹Ÿåˆæ•°æ®ã€‚

è¦æƒ³æœ€å¤§ç¨‹åº¦çš„æ‹Ÿåˆæ•°æ®ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯æ‰¾åˆ°æ²¡æœ‰æ‹Ÿåˆçš„éƒ¨åˆ†ï¼Œå³æŸå¤±çš„éƒ¨åˆ†å°½é‡å°ï¼Œå°±æ˜¯**æŸå¤±å‡½æ•°**ï¼ˆloss functionï¼‰ï¼ˆä¹Ÿæœ‰ç®—æ³•è¡¡é‡æ‹Ÿåˆç¨‹åº¦ï¼Œç§°å‡½æ•°ä¸º**æ•ˆç”¨å‡½æ•°**ï¼ˆutility functionï¼‰ï¼‰ã€‚

æ‰€æœ‰ç®—æ³•æ¨¡åž‹ä¾èµ–äºŽæœ€å°åŒ–æˆ–æœ€å¤§åŒ–æŸä¸€ä¸ªå‡½æ•°ï¼Œç§°ä¹‹ä¸ºâ€œç›®æ ‡å‡½æ•°â€ã€‚

â€œæŸå¤±å‡½æ•°â€æ˜¯æœ€å°åŒ–ä¸€ç»„å‡½æ•°ã€‚æè¿°äº†å•ä¸ªæ ·æœ¬é¢„æµ‹å€¼å’ŒçœŸå®žå€¼ä¹‹é—´è¯¯å·®çš„ç¨‹åº¦ï¼Œç”¨æ¥åº¦é‡æ¨¡åž‹ä¸€æ¬¡é¢„æµ‹çš„å¥½åã€‚

#### æœŸæœ›é£Žé™©

æ˜¯æŸå¤±å‡½æ•°çš„æœŸæœ›ï¼Œè¡¨è¾¾ç†è®ºä¸Šæ¨¡åž‹f(X)å…³äºŽè”åˆåˆ†å¸ƒP(X,Y)çš„å¹³å‡æ„ä¹‰ä¸‹çš„æŸå¤±ï¼Œä¹Ÿå«æœŸæœ›æŸå¤±/é£Žé™©ã€‚

#### ç»éªŒé£Žé™©

æ¨¡åž‹f(X)å…³äºŽè®­ç»ƒæ•°æ®é›†çš„å¹³å‡æŸå¤±ï¼Œç§°ä¸ºç»éªŒé£Žé™©/æŸå¤±ã€‚

#### ç»“æž„é£Žé™©æœ€å°åŒ–

å½“æ ·æœ¬å®¹é‡ä¸å¤§æ—¶ï¼Œç»éªŒé£Žé™©æœ€å°åŒ–å®¹æ˜“â€œè¿‡æ‹Ÿåˆâ€ï¼Œä¸ºå‡ç¼“è¯¥é—®é¢˜ï¼Œæå‡ºç»“æž„é£Žé™©æœ€å°ç†è®ºã€‚ç»“æž„é£Žé™©æœ€å°åŒ–ä¸ºç»éªŒé£Žé™©ä¸Žå¤æ‚åº¦åŒæ—¶è¾ƒå°ã€‚å…¬å¼ä¸Šè¡¨çŽ°ä¸ºï¼Œæ¯”ç»éªŒé£Žé™©å¤šä¸€ä¸ª**æ­£åˆ™åŒ–é¡¹(regularizer)**ï¼Œä¹Ÿå«ç½šé¡¹(penalty)ï¼Œæ˜¯å‡½æ•°çš„å¤æ‚åº¦J(f)*æƒé‡ç³»æ•°Î»ã€‚

>1ã€æŸå¤±å‡½æ•°ï¼šå•ä¸ªæ ·æœ¬é¢„æµ‹å€¼å’ŒçœŸå®žå€¼ä¹‹é—´è¯¯å·®çš„ç¨‹åº¦ã€‚
>
>2ã€æœŸæœ›é£Žé™©ï¼šæ˜¯æŸå¤±å‡½æ•°çš„æœŸæœ›ï¼Œç†è®ºä¸Šæ¨¡åž‹f(X)å…³äºŽè”åˆåˆ†å¸ƒP(X,Y)çš„å¹³å‡æ„ä¹‰ä¸‹çš„æŸå¤±ã€‚
>
>3ã€ç»éªŒé£Žé™©ï¼šæ¨¡åž‹å…³äºŽè®­ç»ƒé›†çš„å¹³å‡æŸå¤±ï¼ˆæ¯ä¸ªæ ·æœ¬çš„æŸå¤±åŠ èµ·æ¥ï¼Œç„¶åŽå¹³å‡ä¸€ä¸‹ï¼‰ã€‚
>
>4ã€ç»“æž„é£Žé™©ï¼šåœ¨ç»éªŒé£Žé™©ä¸ŠåŠ ä¸Šä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆçš„ç­–ç•¥ã€‚



### æœ€å°äºŒä¹˜æ³•

*äºŒä¹˜* æŒ‡**å¹³æ–¹**ã€‚

#### æ€»ç»“

ä¸€ç±»æœºå™¨å­¦ä¹ ç®—æ³•çš„åŸºæœ¬æ€è·¯ï¼š

1. é€šè¿‡åˆ†æžé—®é¢˜ï¼Œç¡®å®šé—®é¢˜çš„æŸå¤±å‡½æ•°/æ•ˆç”¨å‡½æ•°ï¼›
2. é€šè¿‡æœ€ä¼˜åŒ–æŸå¤±/æ•ˆç”¨å‡½æ•°ï¼ŒèŽ·å¾—æœºå™¨å­¦ä¹ çš„æ¨¡åž‹ã€‚

### ä»£ç å®žçŽ°

ä¸åŒçš„æ•°æ®å¯é€‰æ‹©ä¸åŒçš„å‡½æ•°ï¼Œé€šè¿‡æœ€å°äºŒä¹˜æ³•å¾—åˆ°ä¸ä¸€æ ·çš„æ‹Ÿåˆæ›²çº¿ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.array([1.,2.,3.,4.,5.])
y = np.array([1.,3.,2.,3.,5,])

plt.scatter(x,y)
plt.axis([0,6,0,6])
plt.show()

# é¦–å…ˆè¦è®¡ç®—x,yçš„å‡å€¼
x_mean=np.mean(x)
y_mean=np.mean(y)

# açš„åˆ†å­numã€åˆ†æ¯d
num=0.0
d=0.0
for x_i,y_i in zip(x,y): # zipå‡½æ•°æ‰“åŒ…æˆ[(x_i,y_i)...] çš„å½¢å¼
    num=num+(x_i-x_mean)*(y_i-y_mean)
    d=d+(x_i-x_mean)**2
a=num/d
b=y_mean-a*x_mean

# åœ¨æ±‚å‡ºa,båŽï¼Œå¯è®¡ç®—å‡ºyçš„é¢„æµ‹å€¼ï¼Œé¦–å…ˆç»˜åˆ¶æ¨¡åž‹ç›´çº¿ï¼š
y_hat=a*x+b
plt.scatter(x,y) # ç»˜åˆ¶æ•£ç‚¹å›¾
plt.plot(x,y_hat,color='r') # ç»˜åˆ¶ç›´çº¿
plt.axis([0,6,0,6])
plt.show()

# è¿›è¡Œé¢„æµ‹
x_predict=6
y_predict=a*x_predict+b
print(y_predict)
```



å…¶ä¸­

> num=num+(x_i-x_mean)*(y_i-y_mean)
>
> d=d+(x_i-x_mean)**2

å¯çœ‹æˆä¸¤ä¸ªå‘é‡çš„å¯¹åº”é¡¹ç›¸ä¹˜å†ç›¸åŠ ï¼Œå³ä¸¤ä¸ªå‘é‡**â€œç‚¹ä¹˜â€**ã€‚å¯ç”¨numpyä¸­çš„dotè¿ç®—ã€‚

**å‘é‡åŒ–**æ˜¯ååˆ†å¸¸ç”¨çš„åŠ é€Ÿè®¡ç®—æ–¹å¼ï¼Œç‰¹é€‚åˆæ·±åº¦å­¦ä¹ ç­‰éœ€è¦å¤§æ•°æ®çš„é¢†åŸŸã€‚(ç›¸å¯¹çš„ï¼Œforå¾ªçŽ¯æ˜¯å¾ˆæ…¢çš„â€¦â€¦)

#### åˆ›å»ºä¸€ä¸ªSimpleLinearRegression.py

```python
import numpy as np
class SimpleLinearRegression:
  def __init__(self):
    """æ¨¡åž‹åˆå§‹åŒ–å‡½æ•°"""
    self.a_=None
    self.b_=None
    
   def fit(self,x_train,y_train):
    """æ ¹æ®è®­ç»ƒæ•°æ®é›†è®­ç»ƒæ¨¡åž‹"""
    assert x_train.ndim==1 # ç®€å•çº¿æ€§å›žå½’æ¨¡åž‹ä»…èƒ½å¤„ç†ä¸€ç»´å‘é‡ç‰¹å¾
    assert len(x_train)==len(y_train) # ç‰¹å¾å‘é‡çš„é•¿åº¦ä¸Žæ ‡ç­¾çš„é•¿åº¦ç›¸åŒ
    x_mean=np.mean(x_train)
    y_mean=np.mean(y_train)
    num=(x_train-x_mean).dot(y_train-y_mean)
    d=(x_train-x_mean).dot(x_train-x_mean)
    self.a=num/d
    self.b=y_mean-self.a_*x_mean
    
    return self
  
  def predict(self,x_predict):
    """ç»™å®šå¾…é¢„æµ‹æ•°æ®é›†x_predictï¼Œè¿”å›žè¡¨ç¤ºå…¶ç»“æžœå‘é‡"""
    assert x_predict.ndim==1
    assert self.a_ is not None and self.b_ is not None
    return np.array([self.predict(x) for x in x_predict])
  
  def _predict(self,x_single):
    """ç»™å®šå•ä¸ªå¾…é¢„æµ‹æ•°æ®ï¼Œè¿”å›žé¢„æµ‹ç»“æžœå€¼"""
    return self.a_*x_single+self.b_
  
  def __repr__(self):
    """è¿”å›žä¸€ä¸ªå¯ä»¥ç”¨æ¥è¡¨ç¤ºå¯¹è±¡çš„å¯æ‰“å°å­—ç¬¦ä¸²"""
    return "SimpleLinearRegression()"
```



#### è¯„ä»·æŒ‡æ ‡â€”â€”Ræ–¹âž¡ï¸ 1-é¢„æµ‹å€¼ä¸ŽçœŸå®žå€¼ä¹‹å·®çš„å¹³æ–¹/å‡å€¼ä¸ŽçœŸå®žå€¼ä¹‹å·®çš„å¹³æ–¹

é¢„æµ‹å€¼=æ ·æœ¬å‡å€¼ æ˜¯baselineæ¨¡åž‹ï¼Œè¯¥æ¨¡åž‹é”™è¯¯è¾ƒå¤šï¼Œè‡ªå·±çš„æ¨¡åž‹é”™è¯¯è¾ƒå°‘ï¼Œå› æ­¤Ræ–¹è¡¡é‡äº†æ‹Ÿåˆä½æ•°æ®çš„åœ°æ–¹ï¼Œå³*æ²¡æœ‰äº§ç”Ÿé”™è¯¯çš„ç›¸åº”æŒ‡æ ‡*ã€‚å› æ­¤ï¼š

> Ræ–¹è¶Šå¤§ï¼Œé”™è¯¯çŽ‡è¶Šä½Žï¼Œæœ€å¤§å€¼ä¸º1ï¼›
>
> å½“Ræ–¹<0ï¼Œåˆ™è‡ªå·±çš„æ¨¡åž‹ä¸å¦‚åŸºå‡†æ¨¡åž‹ï¼Œå¾ˆå¯èƒ½æ•°æ®ä¸å­˜åœ¨çº¿æ€§å…³ç³»ã€‚



### å¤šå…ƒçº¿æ€§å›žå½’

å¤šå…ƒçº¿æ€§å›žå½’çš„æ­£è§„æ–¹ç¨‹è§£

ç¼ºç‚¹ï¼šæ—¶é—´å¤æ‚åº¦è¾ƒé«˜O(n^3)

ä¼˜ç‚¹ï¼šä¸éœ€å¯¹æ•°æ®å½’ä¸€åŒ–å¤„ç†ï¼ŒåŽŸå§‹æ•°æ®è®¡ç®—å‚æ•°ï¼Œä¸å­˜åœ¨é‡çº²é—®é¢˜

#### ä»£ç å®žçŽ°

```python
import numpy as np
from .metrics import r2_score

class LinearRegression:
  def __init__(self):
    """åˆå§‹åŒ–æ¨¡åž‹"""
    self.coef_=None # ç³»æ•°ï¼ˆtheta0~1å‘é‡ï¼‰
    self.interception_=None # æˆªè·ï¼ˆtheta0æ•°ï¼‰
    self._theta=None # æ•´ä½“è®¡ç®—å‡ºçš„å‘é‡theta
    
   def fit_normal(self,X_train,y_train):
    """æ ¹æ®è®­ç»ƒæ•°æ®è®­ç»ƒæ¨¡åž‹"""
    assert X_train.shape[0]==y_train.shape[0] # sizeå¿…é¡»ç›¸ç­‰
    # æ­£è§„åŒ–æ–¹ç¨‹æ±‚è§£
    X_b=np.hstack([np.ones((len(X_train),1)),X_train])
    self._theta=np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y_train)
      
    self.interception_=self._theta[0]
    self.coef_=self._theta[1:]
    return self
  
  def predict(self,X_predict):
    """ç»™å®šé¢„æµ‹çš„æ•°æ®é›†X_predict,è¿”å›žè¡¨ç¤ºç»“æžœå‘é‡"""
    assert self.interception_ is not None and self.coef_ is not None
    assert X_predict.shape[1]==len(self.coef_)
    X_b=np.hstack([np.ones((len(X_predict),1)),X_predict])
    y_predict=X_b.dot(self._theta)
    return y_predict
  
  def score(self,X_test,y_test):
    """ç¡®å®šå½“å‰æ¨¡åž‹å‡†ç¡®çŽ‡"""
    y_predict=self.predict(self,X_test)
    return r2_score(y_test,y_predict)
  
  def __repr__(self):
    return "LinearRegression()"
      
```

âš ï¸ é”™è¯¯å¾…è§£å†³ðŸ¤”

![84bNMn.png](https://s1.ax1x.com/2020/03/22/84bNMn.png)





